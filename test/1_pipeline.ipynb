{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNdp48aUHHK7g0K7ODD3OyA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GT-Larry/machine-learning/blob/main/test/1_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Face是專注於開發機器學習的工具、應用與模型的一家美國公司。他們最受歡迎的產品之一是開源的transformers庫，這是一個為自然語言處理(NLP)而設計的庫。"
      ],
      "metadata": {
        "id": "W2VquyGs37yd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66n_U6fR16Cp"
      },
      "outputs": [],
      "source": [
        "# 導入transformers函式庫的pipeline函式，它提供簡單的接口，將模型與其必要的處理步驟連起來。\n",
        "from transformers import pipeline\n",
        "\n",
        "# 接著我們就可以運用pipeline來使用模型了。\n",
        "# 舉例，我們需要設定模型的任務是什麼，以下程式碼是將任務設為\"question-answering\"(問答)。\n",
        "# 使用的model部分，可以進行指定。以下程式碼是將model設為\"uer/roberta-base-chinese-extractive-qa\"(源自Hugging Face平台)。\n",
        "# qa_pipe是透過pipeline產生出的物件。\n",
        "qa_pipe = pipeline(task=\"question-answering\", model=\"uer/roberta-base-chinese-extractive-qa\")\n",
        "\n",
        "# 對於qa_pipe，我們提出問題並提供文本，然後模型將從文本中找尋答案來回答問題。\n",
        "# 透過print(response)，可以看到answer為貓頭鷹(問題的答案)。\n",
        "response = qa_pipe(question=\"這動物是什麼？\", context=\"夜晚，樹上有隻貓頭鷹在啼叫\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "如果想知道pipeline可以指定哪些任務，我們可以導入SUPPORTED_TASKS。"
      ],
      "metadata": {
        "id": "25cyAi6TCEjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 從transformers.pipelines導入SUPPORTED_TASKS\n",
        "from transformers.pipelines import SUPPORTED_TASKS\n",
        "\n",
        "# SUPPORTED_TASKS是字典的資料，含有多個key，以及對應每個key的value。\n",
        "# 每個key就是一個任務名稱\n",
        "# 透過for迴圈來印出所有的key(所有任務名稱)\n",
        "for k,v in SUPPORTED_TASKS.items():\n",
        "  print(k)"
      ],
      "metadata": {
        "id": "DgxqHT7XCI2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下是另一種使用pipeline的方式。"
      ],
      "metadata": {
        "id": "HnWOKh2iE87U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#從transformers導入AutoModelForSequenceClassification、AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
        "\n",
        "#Hugging Face 提供的自動載入器，會根據你指定的模型名稱，自動載入一個「序列分類模型」\n",
        "model = AutoModelForSequenceClassification.from_pretrained('liam168/c2-roberta-base-finetuned-dianping-chinese')\n",
        "\n",
        "#自動載入與模型對應的 tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('liam168/c2-roberta-base-finetuned-dianping-chinese')\n",
        "\n",
        "#形成名為pipe的物件\n",
        "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "9w0g21vyE9lG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}